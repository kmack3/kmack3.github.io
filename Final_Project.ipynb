{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Project1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8LHGX-6b25B"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf  # Version 1.0.0\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBs7ITBIcCZH",
        "outputId": "806163ef-4ebc-4688-e13e-6a714e09091d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "MyDrive  Shareddrives\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77Z18Yf84SAy"
      },
      "source": [
        "# Shared Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOPw_QRJ4nOb"
      },
      "source": [
        "## Get the data and split into train + test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci2VCx23cjne",
        "outputId": "2ece18f7-175d-4459-db7c-372bf2b249b3"
      },
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/final_project/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "    print('Creating base path')\n",
        "else: \n",
        "  print('Base path ' + BASE_PATH + ' already exists')\n",
        "\n",
        "os.chdir(BASE_PATH)\n",
        "\n",
        "if not os.path.exists(\"UCI HAR Dataset.zip\"):\n",
        "    print(\"Downloading...\")\n",
        "    !wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"\n",
        "    print(\"Downloading done.\\n\")\n",
        "    !unzip -nq \"UCI HAR Dataset.zip\"\n",
        "else:\n",
        "    print(\"Dataset already downloaded. Did not download twice.\\n\")\n",
        "\n",
        "DATASET_PATH = \"UCI HAR Dataset/\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base path /gdrive/My Drive/colab_files/final_project/ already exists\n",
            "Dataset already downloaded. Did not download twice.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8gf8aOveKS5"
      },
      "source": [
        "# Useful Constants\n",
        "\n",
        "# Those are separate normalised input features for the neural network\n",
        "INPUT_SIGNAL_TYPES = [\n",
        "    \"body_acc_x_\",\n",
        "    \"body_acc_y_\",\n",
        "    \"body_acc_z_\",\n",
        "    \"body_gyro_x_\",\n",
        "    \"body_gyro_y_\",\n",
        "    \"body_gyro_z_\",\n",
        "    \"total_acc_x_\",\n",
        "    \"total_acc_y_\",\n",
        "    \"total_acc_z_\"\n",
        "]\n",
        "\n",
        "# Output classes to learn how to classify\n",
        "LABELS = [\n",
        "    \"WALKING\",\n",
        "    \"WALKING_UPSTAIRS\",\n",
        "    \"WALKING_DOWNSTAIRS\",\n",
        "    \"SITTING\",\n",
        "    \"STANDING\",\n",
        "    \"LAYING\",\n",
        "    \"CANE\"\n",
        "]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAwwQPsselT9"
      },
      "source": [
        "TRAIN = \"train/\"\n",
        "TEST = \"test/\"\n",
        "\n",
        "\n",
        "# Load \"X\" (the neural network's training and testing inputs)\n",
        "\n",
        "def load_X(X_signals_paths):\n",
        "    X_signals = []\n",
        "\n",
        "    for signal_type_path in X_signals_paths:\n",
        "        file = open(signal_type_path, 'r')\n",
        "        # Read dataset from disk, dealing with text files' syntax\n",
        "        X_signals.append(\n",
        "            [np.array(serie, dtype=np.float32) for serie in [\n",
        "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "            ]]\n",
        "        )\n",
        "        file.close()\n",
        "\n",
        "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
        "\n",
        "X_train_signals_paths = [\n",
        "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "X_test_signals_paths = [\n",
        "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "\n",
        "X_train = load_X(X_train_signals_paths)\n",
        "X_test = load_X(X_test_signals_paths)\n",
        "\n",
        "\n",
        "# Load \"y\" (the neural network's training and testing outputs)\n",
        "\n",
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    # Read dataset from disk, dealing with text file's syntax\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]],\n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "\n",
        "    # Substract 1 to each output class for friendly 0-based indexing\n",
        "    return y_ - 1\n",
        "\n",
        "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
        "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
        "\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veLi8ywIfBbd",
        "outputId": "06aee149-cdaf-42ee-99ee-dd76c1707969"
      },
      "source": [
        "print(np.shape(X_train))\n",
        "print(np.shape(y_train))\n",
        "print(np.shape(X_test))\n",
        "print(float(8469504)/7352)\n",
        "print(y_train[83])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7352, 128, 9)\n",
            "(7352, 1)\n",
            "(2947, 128, 9)\n",
            "1152.0\n",
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJKVhqusequ7",
        "outputId": "f1d4880f-7540-4b71-d428-97268cb054a7"
      },
      "source": [
        "# Input Data\n",
        "\n",
        "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
        "test_data_count = len(X_test)  # 2947 testing series\n",
        "n_steps = len(X_train[0])  # 128 timesteps per series\n",
        "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
        "\n",
        "\n",
        "# LSTM Neural Network's internal structure\n",
        "\n",
        "n_hidden = 32 # Hidden layer num of features\n",
        "n_classes = 7 # Total classes (should go up, or should go down)\n",
        "\n",
        "\n",
        "# Training\n",
        "\n",
        "learning_rate = 0.0025\n",
        "lambda_loss_amount = 0.0015\n",
        "training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
        "batch_size = 1500\n",
        "display_iter = 30000  # To show test set accuracy during training\n",
        "\n",
        "\n",
        "# Some debugging info\n",
        "\n",
        "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some useful info to get an insight on dataset's shape and normalisation:\n",
            "(X shape, y shape, every X's mean, every X's standard deviation)\n",
            "(2947, 128, 9) (2947, 1) 0.09913992 0.39567086\n",
            "The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wPQzDF34q35"
      },
      "source": [
        "# LSTM Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkJl5fxZ4cM5"
      },
      "source": [
        "## Create the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSM6cItafA8k"
      },
      "source": [
        "def LSTM_RNN(_X, _weights, _biases):\n",
        "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters.\n",
        "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network.\n",
        "    # Note, some code of this notebook is inspired from an slightly different\n",
        "    # RNN architecture used on another dataset, some of the credits goes to\n",
        "    # \"aymericdamien\" under the MIT license.\n",
        "\n",
        "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
        "    # input shape: (batch_size, n_steps, n_input)\n",
        "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
        "    # Reshape to prepare input to hidden activation\n",
        "    _X = tf.reshape(_X, [-1, n_input])\n",
        "    # new shape: (n_steps*batch_size, n_input)\n",
        "\n",
        "    # ReLU activation, thanks to Yu Zhao for adding this improvement here:\n",
        "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
        "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
        "    _X = tf.split(_X, n_steps, 0)\n",
        "    # new shape: n_steps * (batch_size, n_hidden)\n",
        "\n",
        "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
        "    lstm_cell_1 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cell_2 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
        "    # Get LSTM cell output\n",
        "    outputs, states = tf.compat.v1.nn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
        "\n",
        "    # Get last time step's output feature for a \"many-to-one\" style classifier,\n",
        "    # as in the image describing RNNs at the top of this page\n",
        "    lstm_last_output = outputs[-1]\n",
        "\n",
        "    # Linear activation\n",
        "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
        "\n",
        "\n",
        "def extract_batch_size(_train, step, batch_size):\n",
        "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data.\n",
        "\n",
        "    shape = list(_train.shape)\n",
        "    shape[0] = batch_size\n",
        "    batch_s = np.empty(shape)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Loop index\n",
        "        index = ((step-1)*batch_size + i) % len(_train)\n",
        "        batch_s[i] = _train[index]\n",
        "\n",
        "    return batch_s\n",
        "\n",
        "\n",
        "def one_hot(y_, n_classes=n_classes):\n",
        "    # Function to encode neural one-hot output labels from number indexes\n",
        "    # e.g.:\n",
        "    # one_hot(y_=[[5], [0], [3]], n_classes=6):\n",
        "    #     return [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
        "\n",
        "    y_ = y_.reshape(len(y_))\n",
        "    return np.eye(n_classes)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0d6nljN4iaR"
      },
      "source": [
        "## Train LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwY1CuclfMqi"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Graph input/output\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, n_steps, n_input])\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Graph weights\n",
        "weights = {\n",
        "    'hidden': tf.Variable(tf.random.normal([n_input, n_hidden])), # Hidden layer weights\n",
        "    'out': tf.Variable(tf.random.normal([n_hidden, n_classes], mean=1.0))\n",
        "}\n",
        "biases = {\n",
        "    'hidden': tf.Variable(tf.random.normal([n_hidden])),\n",
        "    'out': tf.Variable(tf.random.normal([n_classes]))\n",
        "}\n",
        "\n",
        "pred = LSTM_RNN(x, weights, biases)\n",
        "\n",
        "# Loss, optimizer and evaluation\n",
        "l2 = lambda_loss_amount * sum(\n",
        "    tf.nn.l2_loss(tf_var) for tf_var in tf.compat.v1.trainable_variables()\n",
        ") # L2 loss prevents this overkill neural network to overfit the data\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXgZnTx5fUdL"
      },
      "source": [
        "# To keep track of training's performance\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Launch the graph\n",
        "sess = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "# Perform Training steps with \"batch_size\" amount of example data at each loop\n",
        "step = 1\n",
        "while step * batch_size <= training_iters:\n",
        "    batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
        "    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
        "\n",
        "    # Fit training using batch data\n",
        "    _, loss, acc = sess.run(\n",
        "        [optimizer, cost, accuracy],\n",
        "        feed_dict={\n",
        "            x: batch_xs,\n",
        "            y: batch_ys\n",
        "        }\n",
        "    )\n",
        "    train_losses.append(loss)\n",
        "    train_accuracies.append(acc)\n",
        "\n",
        "    # Evaluate network only at some steps for faster training:\n",
        "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
        "\n",
        "        # To not spam console, show training accuracy/loss in this \"if\"\n",
        "        print(\"Training iter #\" + str(step*batch_size) + \\\n",
        "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
        "              \", Accuracy = {}\".format(acc))\n",
        "\n",
        "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
        "        loss, acc = sess.run(\n",
        "            [cost, accuracy],\n",
        "            feed_dict={\n",
        "                x: X_test,\n",
        "                y: one_hot(y_test)\n",
        "            }\n",
        "        )\n",
        "        test_losses.append(loss)\n",
        "        test_accuracies.append(acc)\n",
        "        print(\"PERFORMANCE ON TEST SET: \" + \\\n",
        "              \"Batch Loss = {}\".format(loss) + \\\n",
        "              \", Accuracy = {}\".format(acc))\n",
        "\n",
        "    step += 1\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time Taken\")\n",
        "print(end - start)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "\n",
        "# Accuracy for test data\n",
        "\n",
        "one_hot_predictions, accuracy, final_loss = sess.run(\n",
        "    [pred, accuracy, cost],\n",
        "    feed_dict={\n",
        "        x: X_test,\n",
        "        y: one_hot(y_test)\n",
        "    }\n",
        ")\n",
        "\n",
        "test_losses.append(final_loss)\n",
        "test_accuracies.append(accuracy)\n",
        "\n",
        "print(\"FINAL RESULT: \" + \\\n",
        "      \"Batch Loss = {}\".format(final_loss) + \\\n",
        "      \", Accuracy = {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98dTDJrSBLQ3"
      },
      "source": [
        "## Visualize the Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkTMgj6oPuHA"
      },
      "source": [
        "x = np.linspace(1, len(train_losses), len(train_losses))\r\n",
        "\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.plot(x, train_losses, label=\"Train loss\")\r\n",
        "#plt.plot(x, test_losses, label=\"Test loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.title(\"Train and Test Loss\")\r\n",
        "plt.savefig(DATASET_PATH + \"images/lstm_train_loss.png\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "x = np.linspace(1, len(test_losses), len(test_losses))\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "# plt.plot(x, train_losses, label=\"Train loss\")\r\n",
        "plt.plot(x, test_losses, label=\"Test loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.title(\"Train and Test Loss\")\r\n",
        "plt.savefig(DATASET_PATH + \"images/lstm_test_loss.png\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Accuracy\")\r\n",
        "plt.title(\"Test Accuracy\")\r\n",
        "plt.plot(x, test_accuracies)\r\n",
        "plt.savefig(DATASET_PATH + \"images/lstm_test_acc.png\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2KIIxGBBKmh"
      },
      "source": [
        "predictions = one_hot_predictions.argmax(1)\n",
        "\n",
        "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
        "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
        "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"I'm here\")\n",
        "print(y_test.shape)\n",
        "print(y_test[0])\n",
        "print(predictions.shape)\n",
        "print(predictions[0])\n",
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
        "print(confusion_matrix)\n",
        "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
        "\n",
        "print(\"\")\n",
        "print(\"Confusion matrix (normalised to % of total test data):\")\n",
        "print(normalised_confusion_matrix)\n",
        "print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
        "print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
        "\n",
        "# Plot Results:\n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "plt.imshow(\n",
        "    normalised_confusion_matrix,\n",
        "    interpolation='nearest',\n",
        "    cmap=plt.cm.rainbow\n",
        ")\n",
        "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(n_classes)\n",
        "plt.xticks(tick_marks, LABELS, rotation=90)\n",
        "plt.yticks(tick_marks, LABELS)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.savefig(DATASET_PATH + \"images/LSTM_cm.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jh3C0QY4urR"
      },
      "source": [
        "# CNN Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjHcbRRNAREL"
      },
      "source": [
        "## Format the data\n",
        "\n",
        "Use X_train, X_test, y_train, y_test from above. The data is of the shape:\n",
        "\n",
        "[num series, num sensor samples per one series, num sensor streams we are pulling from]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-g4g_kntEea"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66FDCnH1tEmp"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "n_classes = 7\n",
        "class HARDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_array, labels_array, sequence_length, batch_size, num_channels):\n",
        "        super(HARDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        final_data = []\n",
        "        for series in data_array:\n",
        "            transposed = np.transpose(series)\n",
        "            final_data.append(transposed)\n",
        "        final_data = np.array(final_data)\n",
        "        dim1 = final_data.shape[0]\n",
        "        dim2 = final_data.shape[1]\n",
        "        dim3 = final_data.shape[2]\n",
        "        self.final_data = np.resize(final_data, (dim1, dim2, 1, dim3))\n",
        "        self.final_labels = labels_array\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO return the number of unique sequences you have, not the number of characters.\n",
        "        return self.final_data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = torch.from_numpy(self.final_data[idx])\n",
        "        labels = torch.from_numpy(self.final_labels[idx])\n",
        "        return data, labels\n",
        "\n",
        "train_dataset = HARDataset(X_train, y_train, 128, BATCH_SIZE, 9)\n",
        "test_dataset = HARDataset(X_test, y_test, 128, BATCH_SIZE, 9)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjJBIXW0AXbC"
      },
      "source": [
        "## Create the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75FtuYUEAUuE"
      },
      "source": [
        "class MyNet1(nn.Module):\n",
        "    # input is 128x9\n",
        "    def __init__(self):\n",
        "        super(MyNet1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=9, out_channels=3*9, kernel_size=(1,8), groups=9, padding=(0,1))\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=(1,3), stride=2);\n",
        "        self.conv2 = nn.Conv2d(in_channels=27, out_channels=3*27, kernel_size=(1, 4), groups=27, padding=(0,1))\n",
        "        self.fc2 = nn.Linear(4860, n_classes)\n",
        "        self.accuracy = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        label = label.long()\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    # def save_model(self, file_path, num_to_keep=1):\n",
        "    #     pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    # def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "    #     if self.accuracy == None or accuracy > self.accuracy:\n",
        "    #         self.accuracy = accuracy\n",
        "    #         self.save_model(file_path, num_to_keep)\n",
        "\n",
        "    # def load_model(self, file_path):\n",
        "    #     pt_util.restore(self, file_path)\n",
        "\n",
        "    # def load_last_model(self, dir_path):\n",
        "    #     return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXRruWiGrYIX"
      },
      "source": [
        "## Train the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7laDtriAuwND"
      },
      "source": [
        "import time\n",
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    model=model.double()\n",
        "    losses = []\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data.double())\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('{} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                time.ctime(time.time()),\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, log_interval=None):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_predictions, test_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output = model(data.double())\n",
        "            test_loss_on = model.loss(output, label, reduction='sum').item()\n",
        "            test_loss += test_loss_on\n",
        "            pred = output.max(1)[1]\n",
        "            for i in range(0, len(label)):\n",
        "                test_predictions.append(pred[i])\n",
        "                test_labels.append(label[i])\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "            if log_interval is not None and batch_idx % log_interval == 0:\n",
        "                print('{} Test: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    time.ctime(time.time()),\n",
        "                    batch_idx * len(data), len(test_loader.dataset),\n",
        "                    100. * batch_idx / len(test_loader), test_loss_on))\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
        "    return test_loss, test_accuracy, test_predictions, test_labels"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWj3yLaxyscD"
      },
      "source": [
        "# Now the actual training code\n",
        "use_cuda = torch.cuda.is_available()\n",
        "#torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogLTJjrjraHH"
      },
      "source": [
        "def training_loop(model, rate=0.001, epochs=20, momentum=0.9, weight=0.0005, train_dataset=None, test_dataset=None):\n",
        "    print(\"Parameters are rate: %f, epochs %d, momentum %f, weight %f\\n\", rate, epochs, momentum, weight)\n",
        "    # Play around with these constants, you may find a better setting.\n",
        "    EPOCHS = epochs\n",
        "    LEARNING_RATE = rate\n",
        "    MOMENTUM = momentum\n",
        "    USE_CUDA = True\n",
        "    SEED = 0\n",
        "    PRINT_INTERVAL = 100\n",
        "    WEIGHT_DECAY = weight\n",
        "\n",
        "\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "    kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
        "            'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, **kwargs)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "    test_predictions, test_labels = [], []\n",
        "\n",
        "    try:\n",
        "        for epoch in range(0, EPOCHS + 1):\n",
        "            train_loss = train(model, device, train_loader, optimizer, epoch, PRINT_INTERVAL)\n",
        "            print(\"Train loss is \" + str(train_loss))\n",
        "            test_loss, test_accuracy, test_predictions, test_labels = test(model, device, test_loader)\n",
        "            print(\"Test loss is \" + str(test_loss))\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            print(\"Test accuracy is \" + str(test_accuracy))\n",
        "            test_losses.append((epoch, test_loss))\n",
        "            test_accuracies.append((epoch, test_accuracy))\n",
        "            # pt_util.write_log(LOG_PATH + 'log.pkl', (train_losses, test_losses, test_accuracies))\n",
        "            # model.save_best_model(test_accuracy, LOG_PATH + '%03d.pt' % epoch)\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    return train_losses, test_losses, test_accuracies, test_predictions, test_labels"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdeMOmj4x-y8"
      },
      "source": [
        "start = time.time()\n",
        "model = MyNet1().to(device)\n",
        "train_losses1, test_losses1, test_accuracies1, test_predictions1, test_labels1 = training_loop(model, rate=.001, weight=.0005, momentum=.95, epochs=20, train_dataset=train_dataset, test_dataset=test_dataset)\n",
        "end = time.time()\n",
        "print(\"Time Taken2\")\n",
        "print(end - start)\n",
        "# for wd in [.01, .005, .001, .0005, .0001]:\n",
        "# for lr in [.005, .001, .0005, .0001, .0001]:\n",
        "#     model = MyNet1().to(device)\n",
        "#     print(\"Lr is \" + str(lr))\n",
        "#     train_losses, test_losses, test_accuracies, test_predictions, test_labels = training_loop(model, rate=lr, epochs=num_epochs, train_dataset=train_dataset, test_dataset=test_dataset)\n",
        "# for wd in [.01, .005, .001, .0005, .0001]:\n",
        "#     model = MyNet1().to(device)\n",
        "#     print(\"wd is \" + str(wd))\n",
        "#     train_losses, test_losses, test_accuracies, test_predictions, test_labels = training_loop(model, weight=wd, epochs=num_epochs, train_dataset=train_dataset, test_dataset=test_dataset)\n",
        "# for m in [.8, .85, .9, .95]:\n",
        "#     model = MyNet1().to(device)\n",
        "#     print(\"mom is \" + str(m))\n",
        "#     train_losses, test_losses, test_accuracies, test_predictions, test_labels = training_loop(model, momentum=m, epochs=num_epochs, train_dataset=train_dataset, test_dataset=test_dataset)\n",
        "# for e in [15, 20, 25]:\n",
        "#     model = MyNet1().to(device)\n",
        "#     print(\"e is \" + str(e))\n",
        "#     train_losses, test_losses, test_accuracies, test_predictions, test_labels = training_loop(model, epochs=e, train_dataset=train_dataset, test_dataset=test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNBeYKuHtDdG"
      },
      "source": [
        "## Visualize the Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q13XEiYitB7Y"
      },
      "source": [
        "x = np.linspace(1, len(train_loss1), len(train_losses1))\n",
        "train_y = [second for (first,second) in train_losses1]\n",
        "test_loss_y = [second for (first,second) in test_losses1]\n",
        "test_acc_y = [second for (first,second) in test_accuracies1]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(x, test_loss_y, label=\"Test loss\")\n",
        "plt.plot(x, train_y, label=\"Train loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Test Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(DATASET_PATH + \"images/my_orig_loss_better.png\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Test Accuracy\")\n",
        "plt.plot(x, test_acc_y)\n",
        "plt.savefig(DATASET_PATH + \"images/my_orig_acc_better.png\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQyx1hL8DKeQ"
      },
      "source": [
        "print(\"\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"I'm here\")\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(test_labels1, test_predictions1)\n",
        "print(confusion_matrix)\n",
        "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
        "\n",
        "print(\"\")\n",
        "print(\"Confusion matrix (normalised to % of total test data):\")\n",
        "print(normalised_confusion_matrix)\n",
        "\n",
        "# Plot Results:\n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "plt.imshow(\n",
        "    normalised_confusion_matrix,\n",
        "    interpolation='nearest',\n",
        "    cmap=plt.cm.rainbow\n",
        ")\n",
        "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(n_classes)\n",
        "plt.xticks(tick_marks, LABELS, rotation=90)\n",
        "plt.yticks(tick_marks, LABELS)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.savefig(DATASET_PATH + \"images/my_orig_better_cm.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_wqunjA45C1"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9g75mH6p1qR"
      },
      "source": [
        "## Prep new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMxMp4KBpvr6"
      },
      "source": [
        "cane_x_train = []\n",
        "cane_x_test = []\n",
        "waist_x_train = []\n",
        "waist_x_test = []\n",
        "cane_y_train = []\n",
        "cane_y_test = []\n",
        "waist_y_train = []\n",
        "waist_y_test = []\n",
        "\n",
        "\n",
        "files = ['acc_x.txt', 'acc_y.txt', 'acc_z.txt', 'gyro_x.txt', 'gyro_y.txt', 'gyro_z.txt', 'total_x.txt', 'total_y.txt', 'total_z.txt']\n",
        "first_round = True\n",
        "for filename in files:\n",
        "    print(\"Processing file \" + filename)\n",
        "    cane_data = []\n",
        "    waist_data = []\n",
        "\n",
        "    # read in new data\n",
        "    file = open(DATASET_PATH + 'on_cane/' + filename, 'r')\n",
        "    # Read dataset from disk, dealing with text files' syntax\n",
        "    for row in file:\n",
        "        cane_data.append(float(row))\n",
        "    file.close()\n",
        "    file = open(DATASET_PATH + 'in_pocket/' + filename, 'r')\n",
        "    # Read dataset from disk, dealing with text files' syntax\n",
        "    for row in file:\n",
        "        waist_data.append(float(row))\n",
        "    file.close()\n",
        "    \n",
        "    # normalize\n",
        "    cane_normalized = cane_data / np.linalg.norm(cane_data)\n",
        "    waist_normalized = waist_data / np.linalg.norm(waist_data)\n",
        "\n",
        "    # chunk it up into 128 samples\n",
        "    # make shape for CNN: [num_series][channels][128 samples]\n",
        "    # note you need to transpose that for the RNN ^^\n",
        "\n",
        "    # chop off % 128\n",
        "    norm_cane_data = cane_normalized[:-(len(cane_normalized)%128)]\n",
        "    norm_waist_data = waist_normalized[:-(len(waist_normalized)%128)]\n",
        "\n",
        "    # add the overlap between series\n",
        "    cane_data = norm_cane_data[0:64]\n",
        "    waist_data = norm_waist_data[0:64]\n",
        "\n",
        "    stop = False\n",
        "    idx = 128\n",
        "    while(idx < len(norm_cane_data)):\n",
        "        cane_data = np.concatenate((cane_data, norm_cane_data[(idx - 64):idx], norm_cane_data[(idx - 64):idx]))\n",
        "        idx += 64\n",
        "\n",
        "    idx = 128\n",
        "    while(idx < len(norm_waist_data)):\n",
        "        waist_data = np.concatenate((waist_data, norm_waist_data[(idx - 64):idx], norm_waist_data[(idx - 64):idx]))\n",
        "        idx += 64\n",
        "\n",
        "    cane_data = cane_data[0:-64]\n",
        "    waist_data = waist_data[0:-64]\n",
        "\n",
        "    cane_data = np.resize(cane_data, [int(len(cane_data)/128), 128])\n",
        "    waist_data = np.resize(waist_data, [int(len(waist_data)/128), 128])\n",
        "\n",
        "    split_idx_cane = int(len(cane_data)*0.7) # split idx for train and test\n",
        "    split_idx_waist = int(len(waist_data)*0.7)\n",
        "\n",
        "    if first_round:\n",
        "        first_round = False\n",
        "        for i in range(0, cane_data.shape[0]):\n",
        "            if i < split_idx_cane:\n",
        "                cane_x_train.append([])\n",
        "            else:\n",
        "                cane_x_test.append([])\n",
        "        for i in range(0, waist_data.shape[0]):\n",
        "            if i < split_idx_waist:\n",
        "                waist_x_train.append([])\n",
        "            else:\n",
        "                waist_x_test.append([])\n",
        "\n",
        "\n",
        "    for i in range(0, waist_data.shape[0]):\n",
        "        if i < split_idx_waist:\n",
        "            waist_x_train[i].append([waist_data[i]])\n",
        "            waist_y_train.append([6])\n",
        "        else:\n",
        "            waist_x_test[i-split_idx_waist].append([waist_data[i]])\n",
        "            waist_y_test.append([6])\n",
        "\n",
        "    for i in range(0, cane_data.shape[0]):\n",
        "        if i < split_idx_cane:\n",
        "            cane_x_train[i].append([cane_data[i]])\n",
        "            cane_y_train.append([6])\n",
        "        else:\n",
        "            cane_x_test[i-split_idx_cane].append([cane_data[i]])\n",
        "            cane_y_test.append([6])\n",
        "\n",
        "final_data = []\n",
        "for series in X_test:\n",
        "    transposed = np.transpose(series)\n",
        "    final_data.append(transposed)\n",
        "final_data = np.array(final_data)\n",
        "dim1 = final_data.shape[0]\n",
        "dim2 = final_data.shape[1]\n",
        "dim3 = final_data.shape[2]\n",
        "\n",
        "X_test_reshaped = np.resize(final_data, (dim1, dim2, 1, dim3))\n",
        "\n",
        "\n",
        "final_data = []\n",
        "for series in X_train:\n",
        "    transposed = np.transpose(series)\n",
        "    final_data.append(transposed)\n",
        "final_data = np.array(final_data)\n",
        "dim1 = final_data.shape[0]\n",
        "dim2 = final_data.shape[1]\n",
        "dim3 = final_data.shape[2]\n",
        "\n",
        "X_train_reshaped = np.resize(final_data, (dim1, dim2, 1, dim3))\n",
        "\n",
        "\n",
        "cane_x_train = np.array(cane_x_train)\n",
        "cane_x_test = np.concatenate((np.array(cane_x_test), X_test_reshaped))\n",
        "waist_x_train = np.array(waist_x_train)\n",
        "waist_x_test = np.concatenate((np.array(waist_x_test), X_test_reshaped))\n",
        "cane_y_train = np.array(cane_y_train)\n",
        "cane_y_test = np.concatenate((np.array(cane_y_test), y_test))\n",
        "waist_y_train = np.array(waist_y_train)\n",
        "waist_y_test = np.concatenate((np.array(waist_y_test), y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqE0_gKzqaXD"
      },
      "source": [
        "class CaneDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_array, labels_array, sequence_length, batch_size, num_channels):\n",
        "        super(CaneDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        self.final_data = data_array\n",
        "        self.final_labels = labels_array\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO return the number of unique sequences you have, not the number of characters.\n",
        "        return self.final_data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = torch.from_numpy(self.final_data[idx])\n",
        "        labels = torch.from_numpy(self.final_labels[idx])\n",
        "        return data, labels\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvtwSIp3qfgy"
      },
      "source": [
        "class WaistDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_array, labels_array, sequence_length, batch_size, num_channels):\n",
        "        super(WaistDataset, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        self.final_data = data_array\n",
        "        self.final_labels = labels_array\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO return the number of unique sequences you have, not the number of characters.\n",
        "        return self.final_data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = torch.from_numpy(self.final_data[idx])\n",
        "        labels = torch.torch.IntTensor(self.final_labels[idx])\n",
        "        return data, labels\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAsbrEKE_x3Q"
      },
      "source": [
        "## Cane Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxg9BkaVpOIp"
      },
      "source": [
        "train_cane_dataset = CaneDataset(cane_x_train, cane_y_train, 128, BATCH_SIZE, 9)\n",
        "test_cane_dataset = CaneDataset(cane_x_test, cane_y_test, 128, BATCH_SIZE, 9)\n",
        "print(cane_y_train)\n",
        "model = MyNet1().to(device)\n",
        "num_epochs = 20\n",
        "train_loss, test_loss, test_accuracy, test_predictions, test_labels = training_loop(model, epochs=20, train_dataset=train_dataset, test_dataset=test_dataset)\n",
        "\n",
        "# take same model and freeze the n-1 first layers' weights\n",
        "print(model.parameters)\n",
        "model.conv1.requires_grad = False\n",
        "model.maxpool.requires_grad = False\n",
        "model.conv2.requires_grad = False\n",
        "\n",
        "train_loss1, test_loss1, test_accuracy1, test_predictions1, test_labels1 = training_loop(model, rate=.0005, epochs=num_epochs, train_dataset=train_cane_dataset, test_dataset=test_cane_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN2HGHBaQGQ6"
      },
      "source": [
        "x = np.linspace(1, len(train_loss1), len(train_loss1))\r\n",
        "train_y = [second for (first,second) in train_loss1]\r\n",
        "test_loss_y = [second for (first,second) in test_loss1]\r\n",
        "test_acc_y = [second for (first,second) in test_accuracy1]\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.plot(x, test_loss_y, label=\"Test loss\")\r\n",
        "plt.plot(x, train_y, label=\"Train loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.title(\"Train and Test Loss\")\r\n",
        "plt.legend()\r\n",
        "plt.savefig(DATASET_PATH + \"images/cane_loss_lr0005.png\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Accuracy\")\r\n",
        "plt.title(\"Test Accuracy\")\r\n",
        "plt.plot(x, test_acc_y)\r\n",
        "plt.savefig(DATASET_PATH + \"images/cane_acc_lr0005.png\")\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiaPMH45Bp8v"
      },
      "source": [
        "print(\"\")\r\n",
        "print(\"Confusion Matrix:\")\r\n",
        "print(\"I'm here\")\r\n",
        "\r\n",
        "confusion_matrix = metrics.confusion_matrix(test_labels1, test_predictions1)\r\n",
        "print(confusion_matrix)\r\n",
        "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Confusion matrix (normalised to % of total test data):\")\r\n",
        "print(normalised_confusion_matrix)\r\n",
        "\r\n",
        "# Plot Results:\r\n",
        "width = 12\r\n",
        "height = 12\r\n",
        "plt.figure(figsize=(width, height))\r\n",
        "plt.imshow(\r\n",
        "    normalised_confusion_matrix,\r\n",
        "    interpolation='nearest',\r\n",
        "    cmap=plt.cm.rainbow\r\n",
        ")\r\n",
        "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\r\n",
        "plt.colorbar()\r\n",
        "tick_marks = np.arange(n_classes)\r\n",
        "plt.xticks(tick_marks, LABELS, rotation=90)\r\n",
        "plt.yticks(tick_marks, LABELS)\r\n",
        "plt.tight_layout()\r\n",
        "plt.ylabel('True label')\r\n",
        "plt.xlabel('Predicted label')\r\n",
        "plt.savefig(DATASET_PATH + 'images/cane_rate_0005.png')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3AqbSCQ_wng"
      },
      "source": [
        "## Waist Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjZth4FK_wsp"
      },
      "source": [
        "train_dataset = HARDataset(X_train, y_train, 128, BATCH_SIZE, 9)\n",
        "test_dataset = HARDataset(X_test, y_test, 128, BATCH_SIZE, 9)\n",
        "train_waist_datset = WaistDataset(waist_x_train, waist_y_train, 128, BATCH_SIZE, 9)\n",
        "test_waist_dataset = WaistDataset(waist_x_test, waist_y_test, 128, BATCH_SIZE, 9)\n",
        "model2 = MyNet1().to(device)\n",
        "num_epochs = 20\n",
        "train_loss2, test_loss2, test_accuracy2, test_predictions2, test_labels2  = training_loop(model2, epochs=num_epochs, train_dataset=train_dataset, test_dataset=test_dataset)\n",
        "\n",
        "# take same model and freeze the n-1 first layers' weights\n",
        "model2.conv1.requires_grad = False\n",
        "model2.maxpool.requires_grad = False\n",
        "model2.conv2.requires_grad = False\n",
        "\n",
        "train_loss3, test_loss3, test_accuracy3, test_predictions3, test_labels3  = training_loop(model2, weight=.01, epochs=num_epochs, train_dataset=train_waist_datset, test_dataset=test_waist_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTof29HqSs9I"
      },
      "source": [
        "x = np.linspace(1, len(train_loss3), len(train_loss3))\r\n",
        "train_y = [second for (first,second) in train_loss3]\r\n",
        "test_loss_y = [second for (first,second) in test_loss3]\r\n",
        "test_acc_y = [second for (first,second) in test_accuracy3]\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.plot(x, test_loss_y, label=\"Test loss\")\r\n",
        "plt.plot(x, train_y, label=\"Train loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.title(\"Train and Test Loss\")\r\n",
        "plt.legend()\r\n",
        "plt.savefig(DATASET_PATH + \"images/waist_loss_fc2.png\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Accuracy\")\r\n",
        "plt.title(\"Test Accuracy\")\r\n",
        "plt.plot(x, test_acc_y)\r\n",
        "plt.savefig(DATASET_PATH + \"images/waist_acc_fc2.png\")\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaOV-O77fhEu"
      },
      "source": [
        "    print(\"\")\r\n",
        "print(\"Confusion Matrix:\")\r\n",
        "print(\"I'm here\")\r\n",
        "\r\n",
        "confusion_matrix = metrics.confusion_matrix(test_labels3, test_predictions3)\r\n",
        "print(confusion_matrix)\r\n",
        "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Confusion matrix (normalised to % of total test data):\")\r\n",
        "print(normalised_confusion_matrix)\r\n",
        "\r\n",
        "# Plot Results:\r\n",
        "width = 12\r\n",
        "height = 12\r\n",
        "plt.figure(figsize=(width, height))\r\n",
        "plt.imshow(\r\n",
        "    normalised_confusion_matrix,\r\n",
        "    interpolation='nearest',\r\n",
        "    cmap=plt.cm.rainbow\r\n",
        ")\r\n",
        "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\r\n",
        "plt.colorbar()\r\n",
        "tick_marks = np.arange(n_classes)\r\n",
        "plt.xticks(tick_marks, LABELS, rotation=90)\r\n",
        "plt.yticks(tick_marks, LABELS)\r\n",
        "plt.tight_layout()\r\n",
        "plt.ylabel('True label')\r\n",
        "plt.xlabel('Predicted label')\r\n",
        "plt.savefig(DATASET_PATH + 'images/waist_wd01.png')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYcaUtw_2i2J"
      },
      "source": [
        "# Visualize that dataset\r\n",
        "\r\n",
        "Plot signals from the original data and our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU7_vHuvA4-n"
      },
      "source": [
        "## Cane Dataset vs walking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7bjb4U2i_Z"
      },
      "source": [
        "x = np.linspace(1, 128, 128)\r\n",
        "print(y_train[0])\r\n",
        "final_data = []\r\n",
        "for series in X_train:\r\n",
        "        transposed = np.transpose(series)\r\n",
        "        final_data.append(transposed)\r\n",
        "\r\n",
        "to_plot = final_data[83]    # this is a walking sample\r\n",
        "\r\n",
        "my_to_plot = np.resize(cane_x_train[50], (9, 128))\r\n",
        "my_to_plot_waist = np.resize(waist_x_train[20], (9, 128))\r\n",
        "\r\n",
        "titles = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"]\r\n",
        "for i, title in enumerate(titles):\r\n",
        "    y = to_plot[i]\r\n",
        "    my_y = my_to_plot[i]\r\n",
        "    my_y_waist = my_to_plot_waist[i]\r\n",
        "\r\n",
        "    \r\n",
        "    plt.figure()\r\n",
        "    plt.plot(x, y)\r\n",
        "    plt.xlabel(\"Sample number\")\r\n",
        "    plt.ylabel(\"Signal magnitude\")\r\n",
        "    plt.title(title)\r\n",
        "    plt.savefig(DATASET_PATH + 'images/' + title + '.png')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    plt.figure()\r\n",
        "    plt.plot(x, my_y)\r\n",
        "    plt.xlabel(\"Sample number\")\r\n",
        "    plt.ylabel(\"Signal magnitude\")\r\n",
        "    plt.title(\"Cane dataset \" + title)\r\n",
        "    plt.savefig(DATASET_PATH + 'images/cane_data_' + title + '.png')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "    plt.figure()\r\n",
        "    plt.plot(x, my_y_waist)\r\n",
        "    plt.xlabel(\"Sample number\")\r\n",
        "    plt.ylabel(\"Signal magnitude\")\r\n",
        "    plt.title(\"Waist dataset \" + title)\r\n",
        "    plt.savefig(DATASET_PATH + 'images/waist_data_' + title + '.png')\r\n",
        "    plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTtcbAdeA9H4"
      },
      "source": [
        "## Waist Dataset vs walking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-yHunB6A9Qh"
      },
      "source": [
        "x = np.linspace(1, 128, 128)\r\n",
        "print(y_train[0])\r\n",
        "final_data = []\r\n",
        "for series in X_train:\r\n",
        "        transposed = np.transpose(series)\r\n",
        "        final_data.append(transposed)\r\n",
        "\r\n",
        "to_plot = final_data[83]    # this is a walking sample\r\n",
        "\r\n",
        "my_to_plot_waist = np.resize(waist_x_train[20], (9, 128))\r\n",
        "print(my_to_plot_waist)\r\n",
        "titles = [\"acc_x\", \"acc_y\", \"acc_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"]\r\n",
        "for i, title in enumerate(titles):\r\n",
        "    y = to_plot[i]\r\n",
        "    my_y_waist = my_to_plot_waist[i]\r\n",
        "    \r\n",
        "    plt.figure()\r\n",
        "    plt.plot(x, y)\r\n",
        "    plt.xlabel(\"Sample number\")\r\n",
        "    plt.ylabel(\"Signal magnitude\")\r\n",
        "    plt.title(title)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    plt.figure()\r\n",
        "    plt.plot(x, my_y_waist)\r\n",
        "    plt.xlabel(\"Sample number\")\r\n",
        "    plt.ylabel(\"Signal magnitude\")\r\n",
        "    plt.title(\"Waist dataset \" + title)\r\n",
        "    plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}